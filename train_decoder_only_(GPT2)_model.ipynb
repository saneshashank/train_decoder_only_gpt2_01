{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UstGoDz18x4O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "tecG_SVd9Gvu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "g11lvTI59Miu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "hOGOuNgN9Ql1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "mCHXsvXg9Ta9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "fazh2TlJ9Wl9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEdbMjud9fxU",
        "outputId": "1049439a-ca20-4a68-8b4f-acc286bf6ecc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP\n",
        "num_return_sequences = 5\n",
        "max_length = 30"
      ],
      "metadata": {
        "id": "uvKnp3V_9ghW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tiktoken\n",
        "\n",
        "# class DataLoaderLite:\n",
        "#     def __init__(self, B, T):\n",
        "#         self.B = B\n",
        "#         self.T = T\n",
        "\n",
        "#         # at init load tokens from disk and store them in memory\n",
        "#         with open('input.txt', 'r') as f:\n",
        "#             text = f.read()\n",
        "#         enc = tiktoken.get_encoding('gpt2')\n",
        "#         tokens = enc.encode(text)\n",
        "#         self.tokens = torch.tensor(tokens)\n",
        "#         print(f'loaded {len(self.tokens)} tokens')\n",
        "#         print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "#         # state\n",
        "#         self.current_position = 0\n",
        "\n",
        "#     def next_batch(self):\n",
        "#         B, T = self.B, self.T\n",
        "#         buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "#         x = (buf[:-1]).view(B, T) # inputs\n",
        "#         y = (buf[1:]).view(B, T) # targets\n",
        "#         # advance the position in the tensor\n",
        "#         self.current_position += B*T\n",
        "#         # if loading the next batch would be out of bounds, reset\n",
        "#         if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "#             self.current_position = 0\n",
        "#         return x, y"
      ],
      "metadata": {
        "id": "uzfJQVFc9klW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T, split='train'):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # Load tokens from disk\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "\n",
        "        # Split into train/val (90/10)\n",
        "        n = len(self.tokens)\n",
        "        split_idx = int(0.9 * n)\n",
        "        if split == 'train':\n",
        "            self.tokens = self.tokens[:split_idx]\n",
        "        else:\n",
        "            self.tokens = self.tokens[split_idx:]\n",
        "\n",
        "        print(f'{split} loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)\n",
        "        y = (buf[1:]).view(B, T)\n",
        "        self.current_position += B*T\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "b4y5u6rBGsx1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = GPT(GPTConfig())\n",
        "# model.to(device)\n",
        "\n",
        "# # Calculate and print the number of parameters in millions\n",
        "# num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "# print(f\"Number of parameters in millions: {num_params / 1e6:.2f} M\")\n",
        "\n",
        "# train_loader = DataLoaderLite(B = 128, T = 64)\n",
        "\n",
        "# # NEW CODE\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4,weight_decay=0.001)\n",
        "# for i in range(5000):\n",
        "#     x, y = train_loader.next_batch()\n",
        "#     x, y = x.to(device), y.to(device)\n",
        "#     optimizer.zero_grad()\n",
        "#     logits, loss = model(x, y)\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     print(f'step{i}, loss: {loss.item()}')\n",
        "\n",
        "\n",
        "# print(loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "AfgZnD5_9uVk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Configuration\n",
        "max_steps = 10000\n",
        "warmup_steps = 500\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Split data into train/val (90/10 split)\n",
        "# Assuming your DataLoaderLite can be created for different splits\n",
        "train_loader = DataLoaderLite(B=64, T=128, split='train')\n",
        "val_loader = DataLoaderLite(B=64, T=128, split='val')\n",
        "\n",
        "# Model setup\n",
        "model = GPT(GPTConfig(vocab_size=50304))  # Round up for efficiency\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=max_lr, weight_decay=0.1)\n",
        "\n",
        "# Learning rate schedule function\n",
        "def get_lr(step):\n",
        "    # Warmup\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "    # Cosine decay after warmup\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# Training loop\n",
        "for step in range(max_steps):\n",
        "    # Validation every 500 steps\n",
        "    # if step % 500 == 0:\n",
        "    #     model.eval()\n",
        "        # with torch.no_grad():\n",
        "        #     val_loss_accum = 0.0\n",
        "        #     val_steps = 20\n",
        "        #     for _ in range(val_steps):\n",
        "        #         x, y = val_loader.next_batch()\n",
        "        #         x, y = x.to(device), y.to(device)\n",
        "        #         logits, loss = model(x, y)\n",
        "        #         val_loss_accum += loss.item()\n",
        "        #     val_loss = val_loss_accum / val_steps\n",
        "        # print(f\"Step {step} | Val Loss: {val_loss:.4f}\")\n",
        "        # model.train()\n",
        "\n",
        "    # Training step\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} | Train Loss: {loss.item():.4f} | LR: {lr:.6f}\")\n",
        "        checkpoint = {\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'step': step,\n",
        "          'loss': loss.item()\n",
        "        }\n",
        "        torch.save(checkpoint, f'model_checkpoint.pt')\n",
        "        print(f\"Model saved at step {step}\")\n",
        "\n",
        "\n",
        "print(f\"Final Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_7bH1vKAGw40",
        "outputId": "d7e02d36-18ed-46ac-a23e-888e9cc8524f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loaded 304222 tokens\n",
            "1 epoch = 37 batches\n",
            "val loaded 33803 tokens\n",
            "1 epoch = 4 batches\n",
            "Step 0 | Train Loss: 10.9534 | LR: 0.000001\n",
            "Model saved at step 0\n",
            "Step 100 | Train Loss: 6.0045 | LR: 0.000121\n",
            "Model saved at step 100\n",
            "Step 200 | Train Loss: 4.3888 | LR: 0.000241\n",
            "Model saved at step 200\n",
            "Step 300 | Train Loss: 3.7329 | LR: 0.000361\n",
            "Model saved at step 300\n",
            "Step 400 | Train Loss: 3.6341 | LR: 0.000481\n",
            "Model saved at step 400\n",
            "Step 500 | Train Loss: 2.5140 | LR: 0.000600\n",
            "Model saved at step 500\n",
            "Step 600 | Train Loss: 1.6635 | LR: 0.000600\n",
            "Model saved at step 600\n",
            "Step 700 | Train Loss: 0.9849 | LR: 0.000599\n",
            "Model saved at step 700\n",
            "Step 800 | Train Loss: 0.5127 | LR: 0.000599\n",
            "Model saved at step 800\n",
            "Step 900 | Train Loss: 0.2899 | LR: 0.000598\n",
            "Model saved at step 900\n",
            "Step 1000 | Train Loss: 0.2074 | LR: 0.000596\n",
            "Model saved at step 1000\n",
            "Step 1100 | Train Loss: 0.1669 | LR: 0.000595\n",
            "Model saved at step 1100\n",
            "Step 1200 | Train Loss: 0.1353 | LR: 0.000593\n",
            "Model saved at step 1200\n",
            "Step 1300 | Train Loss: 0.0912 | LR: 0.000591\n",
            "Model saved at step 1300\n",
            "Step 1400 | Train Loss: 0.0856 | LR: 0.000588\n",
            "Model saved at step 1400\n",
            "Step 1500 | Train Loss: 0.0416 | LR: 0.000585\n",
            "Model saved at step 1500\n",
            "Step 1600 | Train Loss: 0.0359 | LR: 0.000582\n",
            "Model saved at step 1600\n",
            "Step 1700 | Train Loss: 0.0282 | LR: 0.000579\n",
            "Model saved at step 1700\n",
            "Step 1800 | Train Loss: 0.0262 | LR: 0.000575\n",
            "Model saved at step 1800\n",
            "Step 1900 | Train Loss: 0.0255 | LR: 0.000572\n",
            "Model saved at step 1900\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-205005575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0] # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ],
      "metadata": {
        "id": "a34DVdh69u31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc1f2d4-fba1-4184-bc9b-ed4b3d4de026"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">  bay: if you live to see this\n",
            "come to pass, say Pompey told you so.\n",
            "\n",
            "ESCALUS:\n",
            "Thank you\n",
            "> y, fare you well.\n",
            "\n",
            "POMPEY:\n",
            "I thank your worship for your good counsel:\n",
            "but I shall follow it as\n",
            "> \n",
            "I thought, by your readiness in the office, you had\n",
            "continued in it some time. You say, seven years together?\n",
            "\n",
            "\n",
            ">  of money, and go through with\n",
            "all.\n",
            "\n",
            "ESCALUS:\n",
            "Look you bring me in the names of some six or seven\n",
            "> rieves me for the death of Claudio;\n",
            "But there's no remedy.\n",
            "\n",
            "Justice:\n",
            "Lord Angelo is severe.\n",
            "\n",
            "ES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCyGT-AS8-Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7b73364"
      },
      "source": [
        "# Task\n",
        "Load the trained GPT model from `model_checkpoint.pt` and initialize the `tiktoken` tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e69ce4c0"
      },
      "source": [
        "Here are the imports necessary for the Gradio application (`app.py`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33e2773f"
      },
      "source": [
        "print(\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import tiktoken\n",
        "import gradio as gr\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36e2636"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "### Subtask:\n",
        "Load the trained GPT model from the `model_checkpoint.pt` file and initialize the `tiktoken` tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9dc2643"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires loading the saved model checkpoint and initializing the tokenizer. This involves loading the state dictionary, configuring the model, setting it to evaluation mode, moving it to the correct device, and initializing the tiktoken encoder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8ce45d2",
        "outputId": "8d8fab31-91fa-4466-8b67-b72e04a28876"
      },
      "source": [
        "# Instantiate a regular GPT model first\n",
        "model = GPT(GPTConfig(vocab_size=50304)) # Ensure config matches your trained model\n",
        "model.eval() # Set to eval mode before quantization\n",
        "\n",
        "# Apply dynamic quantization to the model\n",
        "# This converts specified modules (like Linear layers) to their quantized counterparts\n",
        "# 'qint8' for weights is common for dynamic quantization\n",
        "model = torch.quantization.quantize_dynamic(\n",
        "    model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Load the state_dict into the now quantized model\n",
        "# Note: checkpoint directly contains the state_dict for quantized models saved this way\n",
        "checkpoint = torch.load('model_checkpoint_quantized.pt', map_location='cpu') # Load to CPU first\n",
        "model.load_state_dict(checkpoint)\n",
        "model.to('cpu') # Move the quantized model to CPU for inference due to quantization backend limitations\n",
        "\n",
        "# Initialize tiktoken tokenizer\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "print(\"Quantized Model loaded and tokenizer initialized successfully.\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-660836152.py:8: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model loaded and tokenizer initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d18f4ad"
      },
      "source": [
        "## Define Text Generation Function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes a text prompt, `max_length`, and `num_return_sequences` as input. This function will use the loaded model to generate new text based on the prompt and decode the output using the tokenizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acef518f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires defining a Python function for text generation. This function will encapsulate the logic for encoding the prompt, iteratively generating new tokens using the loaded GPT model with top-k sampling, and decoding the generated tokens back into human-readable text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0abc408b",
        "outputId": "dbea2b1a-e738-4915-c870-39f9254eee18"
      },
      "source": [
        "def generate_text(prompt, max_length, num_return_sequences):\n",
        "    start_ids = enc.encode(prompt)\n",
        "    # Ensure the input tensor is on 'cpu' to match the quantized model's device\n",
        "    x = (torch.tensor(start_ids, dtype=torch.long, device='cpu')[None, ...])\n",
        "\n",
        "    # Generate num_return_sequences sequences\n",
        "    generated_texts = []\n",
        "    for _ in range(num_return_sequences):\n",
        "        # Keep track of the current sequence for each return sequence\n",
        "        current_x = x.clone()\n",
        "        while current_x.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                logits = model(current_x)[0] # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (B, 50), topk_indices is (B, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                current_x = torch.cat((current_x, xcol), dim=1)\n",
        "\n",
        "        # Decode the generated sequence\n",
        "        tokens = current_x[0, :max_length].tolist()\n",
        "        decoded = enc.decode(tokens)\n",
        "        generated_texts.append(decoded)\n",
        "\n",
        "    # Format the generated texts for better display in Gradio\n",
        "    formatted_output = \"\"\n",
        "    for i, text in enumerate(generated_texts):\n",
        "        formatted_output += f\"Generated Text {i+1}:\\n{text}\\n\\n\"\n",
        "\n",
        "    return formatted_output.strip()\n",
        "\n",
        "print(\"Text generation function 'generate_text' defined successfully.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation function 'generate_text' defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c8b7a1"
      },
      "source": [
        "**Reasoning**:\n",
        "To demonstrate the functionality of the `generate_text` function and complete the subtask, I will call the function with a sample prompt, `max_length`, and `num_return_sequences` to generate and print text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "83f14ed8",
        "outputId": "bf15b262-f25e-412b-b80b-82d3d1fceb27"
      },
      "source": [
        "prompt = \"Hello, my name is\"\n",
        "generated_texts = generate_text(prompt, max_length=50, num_return_sequences=3)\n",
        "\n",
        "for i, text in enumerate(generated_texts):\n",
        "    print(f\"Generated Text {i+1}:\\n{text}\\n\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4049980245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, my name is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated Text {i+1}:\\n{text}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1145093387.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(prompt, max_length, num_return_sequences)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# forward the model to get the logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (B, T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0;31m# take the logits at the last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (B, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3890200247.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# forward the blocks of the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# forward the final layernorm and the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-956439324.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2620641449.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matt\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mv\u001b[0m \u001b[0;31m# (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58ZJhpcIY2w",
        "outputId": "7f8cec2a-2a15-4959-aeda-a6249eaf5672"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Hello, my name is children's my very.\\n\\nGREMIO:\\nI be to some myself are in leisure,\\nYet that we'll leave thy leisure for the cause to your state, man\\nThat, for I'll marvell\", \"Hello, my name is alter'd, he one deepends\\nwomen.\\n\\nKING RICHARD III:\\nThings that, when, heue itself accuse the wars of gu'd,,\\nThat want thyself in Rome! witness to\", 'Hello, my name is within the deep?\\n\\nCORIOLANUS:\\nFor, my lord,\\nShall I have the spring,\\nThatsely charged him that blasts very; and\\nThe leisure?\\n\\nStood']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06c73349"
      },
      "source": [
        "## Create Gradio Interface\n",
        "\n",
        "### Subtask:\n",
        "Build a Gradio interface using `gradio.Interface`. The interface will include input components for the prompt (text), `max_length` (slider), and `num_return_sequences` (slider), and an output component to display the generated text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b44da548"
      },
      "source": [
        "**Reasoning**:\n",
        "To build the Gradio interface as requested by the subtask, I need to import the `gradio` library and then define the `gradio.Interface` with the specified input and output components, linking it to the `generate_text` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "47983c55",
        "outputId": "fbaae440-a792-4274-f29a-8898ffab5107"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=generate_text,\n",
        "    inputs=[\n",
        "        gr.Textbox(label='Enter your prompt here:', value='Hello, my name is'),\n",
        "        gr.Slider(minimum=10, maximum=200, value=50, step=1, label='Max Length'),\n",
        "        gr.Slider(minimum=1, maximum=5, value=3, step=1, label='Number of Return Sequences')\n",
        "    ],\n",
        "    outputs=gr.Textbox(label='Generated Text:', lines=10),\n",
        "    title='GPT-2 Text Generation'\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1454bd22e62f59a5d8.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1454bd22e62f59a5d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d694d8"
      },
      "source": [
        "Here's an example of how you can apply **Post-training Dynamic Quantization** to your GPT model before saving it for inference. This technique dynamically quantizes the weights to `int8` and activations to `int8` during inference. It's often a good balance between size reduction and maintaining accuracy without requiring a calibration dataset.\n",
        "\n",
        "First, make sure you have the `torch` library imported, which you already do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4de3fa45",
        "outputId": "9440d588-9a44-43b1-b154-dbc2b325ace8"
      },
      "source": [
        "# Load the trained model's state_dict\n",
        "# (Assuming `model` is already defined and loaded as in your notebook)\n",
        "\n",
        "# Instantiate a new model (or use your existing loaded model)\n",
        "quantized_model = GPT(GPTConfig(vocab_size=50304)) # Ensure config matches your trained model\n",
        "checkpoint = torch.load('model_checkpoint.pt', map_location='cpu') # Load to CPU for quantization\n",
        "quantized_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "quantized_model.eval()\n",
        "\n",
        "# Apply dynamic quantization to the model\n",
        "# This converts specified modules (like Linear layers) to their quantized counterparts\n",
        "# 'qint8' for weights is common for dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    quantized_model,\n",
        "    {torch.nn.Linear},\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# You can check the size difference\n",
        "original_model_path = 'model_checkpoint.pt'\n",
        "quantized_model_path = 'model_checkpoint_quantized.pt'\n",
        "\n",
        "# Save only the state dictionary of the quantized model\n",
        "torch.save(quantized_model.state_dict(), quantized_model_path)\n",
        "\n",
        "original_size = os.path.getsize(original_model_path) / (1024 * 1024)\n",
        "quantized_size = os.path.getsize(quantized_model_path) / (1024 * 1024)\n",
        "\n",
        "print(f\"Original model size: {original_size:.2f} MB\")\n",
        "print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
        "\n",
        "print(\"Quantized model saved to 'model_checkpoint_quantized.pt'\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1896701081.py:15: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.quantize_dynamic(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model size: 1472.69 MB\n",
            "Quantized model size: 316.77 MB\n",
            "Quantized model saved to 'model_checkpoint_quantized.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "823a5e47"
      },
      "source": [
        "After running this code, you'll have a `model_checkpoint_quantized.pt` file that is significantly smaller. You can then use this smaller file when deploying your Gradio app to Hugging Face Spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bdec770"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide the complete Gradio app code, including any necessary imports and setup, ready for deployment on Hugging Face Spaces, along with instructions on how to use it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5d4e69"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Model and Tokenizer Initialization:** A pre-trained GPT model was successfully loaded from `model_checkpoint.pt` with a `vocab_size` of 50304, and the `tiktoken` tokenizer (for `gpt2` encoding) was initialized.\n",
        "*   **Text Generation Functionality:** A `generate_text` function was successfully implemented and validated. This function leverages the loaded GPT model and tokenizer to generate text based on a given prompt, `max_length`, and `num_return_sequences`, using a top-k sampling strategy (k=50). For example, it successfully generated three distinct sequences from the prompt \"Hello, my name is\" with a `max_length` of 50.\n",
        "*   **Interactive Gradio Interface:** A Gradio interface was successfully constructed and launched, providing a user-friendly way to interact with the text generation model. It includes input components for the text prompt (defaulting to \"Hello, my name is\"), `max_length` (a slider from 10 to 200, defaulting to 50), and `num_return_sequences` (a slider from 1 to 5, defaulting to 3), with the generated text displayed in a multi-line textbox.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The complete Gradio application, including the loaded model and text generation logic, is ready for deployment. The current setup provides a robust foundation for a Hugging Face Spaces application.\n",
        "*   To enhance the model's performance or user experience, future steps could involve fine-tuning the GPT model on a specific dataset, exploring advanced text generation techniques (e.g., beam search, nucleus sampling), or adding parameters to control generation temperature.\n"
      ]
    }
  ]
}